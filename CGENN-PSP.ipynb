{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cloning into CGENNs repo"
      ],
      "metadata": {
        "id": "PAJmCur_yV5H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1PC0_K10t2F"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/DavidRuhe/clifford-group-equivariant-neural-networks.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIidLReQDX6S"
      },
      "outputs": [],
      "source": [
        "cd /content/clifford-group-equivariant-neural-networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_j8a2ZppE6fj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from models.modules.fcgp import FullyConnectedSteerableGeometricProductLayer\n",
        "from models.modules.gp import SteerableGeometricProductLayer\n",
        "from models.modules.linear import MVLinear\n",
        "from models.modules.mvsilu import MVSiLU\n",
        "from models.modules.mvlayernorm import MVLayerNorm\n",
        "from models.modules.normalization import NormalizationLayer\n",
        "\n",
        "from algebra.cliffordalgebra import CliffordAlgebra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rycigOpGE5hs"
      },
      "outputs": [],
      "source": [
        "algebra = CliffordAlgebra((1., 1., 1.))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CSDoJAqINRL"
      },
      "outputs": [],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZQdopR3DZ7G"
      },
      "source": [
        "# Fetching data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMhZj0ZUNJck"
      },
      "outputs": [],
      "source": [
        "! wget http://deep.cs.umsl.edu/pdnet/train-data.tar.gz\n",
        "! tar zxf train-data.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x93wrLcoZ7bq"
      },
      "outputs": [],
      "source": [
        "!pip install graph-transformer-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mltpyJyADLXA"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-msssim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTmqVTqGKOpJ"
      },
      "outputs": [],
      "source": [
        "!pip install pycuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_gTMkOMvhBt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCSZGfYYNMUM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import random\n",
        "from pytorch_msssim import SSIM\n",
        "import gc\n",
        "from graph_transformer_pytorch import GraphTransformer\n",
        "import time\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VseXyVvJHTIC"
      },
      "outputs": [],
      "source": [
        "version = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sxzht4ePoM1"
      },
      "outputs": [],
      "source": [
        "def get_nodes_and_edges(pdb, all_feat_paths, node_n, edge_n):\n",
        "    features = None\n",
        "    for path in all_feat_paths:\n",
        "        if os.path.exists(path + pdb + '.pkl'):\n",
        "            features = pickle.load(open(path + pdb + '.pkl', 'rb'))\n",
        "    l = len(features['seq'])\n",
        "    seq = features['seq']\n",
        "\n",
        "    nodes = torch.zeros(1, l, node_n)\n",
        "    edges = torch.zeros(1, l, l, edge_n)\n",
        "    mask = torch.ones(1, l).bool()\n",
        "\n",
        "    ######NODES FEATURES########\n",
        "\n",
        "    # Add secondary structure\n",
        "    ss = features['ss']\n",
        "    assert ss.shape == (3, l)\n",
        "    fi = 0\n",
        "    gi = 0\n",
        "\n",
        "    for j in range(3):\n",
        "        a = np.repeat(ss[j].reshape(1, l), l, axis = 0)\n",
        "        a = a[0,0:l]\n",
        "        np.reshape(a, [1, l])\n",
        "        nodes[:, :, fi] = torch.from_numpy(a).to(nodes)\n",
        "        fi += 1\n",
        "    # Add PSSM\n",
        "    pssm = features['pssm']\n",
        "    assert pssm.shape == (l, 22)\n",
        "    for j in range(22):\n",
        "        a = np.repeat(pssm[:, j].reshape(1, l), l, axis = 0)\n",
        "        a = a[0,0:l]\n",
        "        np.reshape(a, [1, l])\n",
        "        nodes[:, :, fi] = torch.from_numpy(a).to(nodes)\n",
        "        fi += 1\n",
        "\n",
        "    # Add SA\n",
        "    sa = features['sa']\n",
        "    assert sa.shape == (l, )\n",
        "    a = np.repeat(sa.reshape(1, l), l, axis = 0)\n",
        "    a = a[0,0:l]\n",
        "    np.reshape(a, [1, l])\n",
        "    nodes[:, :, fi] = torch.from_numpy(a).to(nodes)\n",
        "    fi += 1\n",
        "\n",
        "    # Add entropy\n",
        "    entropy = features['entropy']\n",
        "    assert entropy.shape == (l, )\n",
        "    a = np.repeat(entropy.reshape(1, l), l, axis = 0)\n",
        "    a = a[0,0:l]\n",
        "    np.reshape(a, [1, l])\n",
        "    nodes[:, :, fi] = torch.from_numpy(a).to(nodes)\n",
        "    fi += 1\n",
        "\n",
        "    ######EDGES FEATURES########\n",
        "\n",
        "    # Add CCMpred\n",
        "    ccmpred = features['ccmpred']\n",
        "    assert ccmpred.shape == ((l, l))\n",
        "    edges[:, :, :, gi] = torch.from_numpy(ccmpred).to(edges)\n",
        "    gi += 1\n",
        "    # Add  FreeContact\n",
        "    freecon = features['freecon']\n",
        "    assert freecon.shape == ((l, l))\n",
        "    edges[:, :, :, gi] = torch.from_numpy(freecon).to(edges)\n",
        "    gi += 1\n",
        "    # Add potential\n",
        "    potential = features['potential']\n",
        "    assert potential.shape == ((l, l))\n",
        "    edges[:, :, :, gi] = torch.from_numpy(potential).to(edges)\n",
        "    gi += 1\n",
        "\n",
        "    if version == 1:\n",
        "        cost = np.load('drive/MyDrive/Output-ALL/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "    if version == 2:\n",
        "        cost = np.load('drive/MyDrive/OrientedPoints/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "    if version == 3:\n",
        "\n",
        "        cost = np.load('drive/MyDrive/OrientedPoints/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "        cost = np.load('drive/MyDrive/Output-ALL/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "    if version == 4:\n",
        "\n",
        "        cost = np.load('drive/MyDrive/Phi/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        cost[2] = np.nan_to_num(cost[2])\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "        cost = np.load('drive/MyDrive/Psi/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        cost[2] = np.nan_to_num(cost[2])\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "        cost = np.load('drive/MyDrive/Omega/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        cost[2] = np.nan_to_num(cost[2])\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "    if version == 5:\n",
        "\n",
        "        cost = np.load('drive/MyDrive/Output-ALL/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "        cost = np.load('drive/MyDrive/Cost_CB/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "    if version == 6:\n",
        "\n",
        "        cost = np.load('drive/MyDrive/OrientedPoints/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "        cost = np.load('drive/MyDrive/OrientedPoints_CB/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "    if version == 7:\n",
        "\n",
        "        cost = np.load('drive/MyDrive/OrientedPoints/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "        cost = np.load('drive/MyDrive/OrientedPoints_CB/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "        cost = np.load('drive/MyDrive/OrientedPoints_C/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "    if version == 8:\n",
        "\n",
        "        cost = np.load('drive/MyDrive/OrientedPoints/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "        cost = np.load('drive/MyDrive/OrientedPoints_CB/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "        cost = np.load('drive/MyDrive/OrientedPoints_N/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "    if version == 9:\n",
        "\n",
        "        cost = np.load('drive/MyDrive/Output-ALL/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "        cost = np.load('drive/MyDrive/OrientedPoints/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "        cost = np.load('drive/MyDrive/OrientedPoints_CB/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "    if version == 10:\n",
        "\n",
        "        cost = np.load('drive/MyDrive/Output-ALL/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "        cost = np.load('drive/MyDrive/OrientedPoints_CB/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "        cost = np.load('drive/MyDrive/OrientedPoints_N/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "    if version == 11:\n",
        "\n",
        "        cost = np.load('drive/MyDrive/Cost_phi/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "        cost = np.load('drive/MyDrive/Cost_psi/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "        cost = np.load('drive/MyDrive/Cost_omega/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "        edges[:, :, :, gi] = torch.from_numpy(cost[2]).to(edges)\n",
        "        gi += 1\n",
        "\n",
        "    distance = np.load('drive/MyDrive/DISTANCES/'+ pdb + '-cb.npy', allow_pickle = True)\n",
        "    edges[:, :, :, gi] = torch.from_numpy(distance[2]).to(edges)\n",
        "\n",
        "    return nodes, edges, mask, l\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQVvZF5cMpCI"
      },
      "source": [
        "#Defining the GrT + 3D Projector architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YwExhQQKD1O"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.fc1 = nn.Linear(27,3, bias = False)\n",
        "      #self.fc1 = nn.Linear(27, 3*128, bias = False)\n",
        "\n",
        "      #self.cgenn1 = FullyConnectedSteerableGeometricProductLayer(algebra=algebra, in_features=9, out_features=3)\n",
        "      #self.cgenn2 = FullyConnectedSteerableGeometricProductLayer(algebra=algebra, in_features=3, out_features=1)\n",
        "\n",
        "      self.cgenn1 = MVLinear(algebra=algebra, in_features=9, out_features=3)\n",
        "      self.cgenn2 = MVLinear(algebra=algebra, in_features=3, out_features=1)\n",
        "\n",
        "      self.norm1 = MVLayerNorm(algebra, 1)\n",
        "      self.act1 = MVSiLU(algebra, 1)\n",
        "\n",
        "    def forward(self, x, old):\n",
        "      #output = self.fc1(x)\n",
        "      output = torch.reshape(x, (-1, 9, 3))\n",
        "      output = algebra.embed_grade(output, 1)\n",
        "\n",
        "      output = self.cgenn1(output)\n",
        "\n",
        "      output = algebra.get_grade(output, 1)\n",
        "      output = algebra.embed_grade(output, 1)\n",
        "\n",
        "      #output = self.norm1(output)\n",
        "      #output = self.cgenn2(output)\n",
        "\n",
        "      output = algebra.get_grade(output, 1)\n",
        "      output = torch.reshape(output, (1, -1, 3))\n",
        "      return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgVdLZqS67vJ"
      },
      "outputs": [],
      "source": [
        "node_n = 27\n",
        "\n",
        "if version == 0:\n",
        "    edge_n = 4\n",
        "if version == 1 or version == 2:\n",
        "    edge_n = 5\n",
        "if version == 3:\n",
        "    edge_n = 6\n",
        "if version == 4:\n",
        "    edge_n = 7\n",
        "if version == 5:\n",
        "    edge_n = 6\n",
        "if version == 6:\n",
        "    edge_n = 6\n",
        "if version == 7 or version == 8 or version == 9 or version == 10 or version == 11:\n",
        "    edge_n = 7\n",
        "\n",
        "GT = GraphTransformer(\n",
        "    dim = node_n,\n",
        "    depth = 3,\n",
        "    heads = 4,\n",
        "    edge_dim = edge_n,             # optional - if left out, edge dimensions is assumed to be the same as the node dimensions above\n",
        "    with_feedforwards = True,   # whether to add a feedforward after each attention layer, suggested by literature to be needed\n",
        "    gated_residual = True,      # to use the gated residual to prevent over-smoothing\n",
        "    rel_pos_emb = True          # set to True if the nodes are ordered, default to False\n",
        ")\n",
        "\n",
        "projector3D = Net()\n",
        "\n",
        "#GT.cuda()\n",
        "#projector3D.cuda()\n",
        "model = nn.Sequential(GT, projector3D)\n",
        "#model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "oQGKX3K3ykO4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5QIMD-Tuj-v"
      },
      "outputs": [],
      "source": [
        "class EarlyStopper:\n",
        "    def __init__(self, patience=1, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = np.inf\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8CiNUkCZW8X"
      },
      "outputs": [],
      "source": [
        "dir_dataset = './data'\n",
        "path = dir_dataset + '/deepcov/features/'\n",
        "\n",
        "dir = '/content/drive/MyDrive/DISTANCES/'\n",
        "lst = os.listdir(dir)\n",
        "lst.sort()\n",
        "\n",
        "lst_train = []\n",
        "for filename in lst:\n",
        "    pdb = os.path.splitext(filename)[0]\n",
        "    pdb = pdb[:-3]\n",
        "    if os.path.exists(path + pdb + '.pkl'):\n",
        "        lst_train = np.append(lst_train, filename)\n",
        "\n",
        "\n",
        "path = dir_dataset + '/psicov/features/'\n",
        "\n",
        "lst_test = []\n",
        "for filename in lst:\n",
        "    pdb = os.path.splitext(filename)[0]\n",
        "    pdb = pdb[:-3]\n",
        "    if os.path.exists(path + pdb + '.pkl'):\n",
        "        lst_test = np.append(lst_test, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztGm54H59fZY"
      },
      "outputs": [],
      "source": [
        "PATH = \"drive/MyDrive/TripleFCGP/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K57DVQWam80"
      },
      "outputs": [],
      "source": [
        "dir = '/content/drive/MyDrive/DISTANCES/'\n",
        "lst = os.listdir(dir)\n",
        "lst.sort()\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "#GT.to(device)\n",
        "#projector3D.to(device)\n",
        "model.to(device)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "dir_dataset = './data/'\n",
        "train_feat_paths = [dir_dataset + '/deepcov/features/']\n",
        "test_dist_paths = [dir_dataset + '/psicov/distance/']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, verbose = True)\n",
        "\n",
        "early_stopper = EarlyStopper(patience=4, min_delta=0.1)\n",
        "stopflag = 0\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "lsttrain, lstval =  train_test_split(lst_train, test_size=0.20, random_state=42)\n",
        "\n",
        "#lsttrain = lst_train[:500]\n",
        "#lstval = lst_train[500:505]\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 8\n",
        "#n_batches = len(lsttrain) // batch_size\n",
        "n_batches =  len(lsttrain) // batch_size\n",
        "\n",
        "\n",
        "if os.path.isfile(PATH + \"BEST_GTmodel.pt\"):\n",
        "    checkpoint = torch.load(PATH + \"BEST_GTmodel.pt\")\n",
        "    minimum = checkpoint['minimum']\n",
        "else:\n",
        "    minimum= 100000\n",
        "\n",
        "print(\"The minimum validation loss is: \", minimum)\n",
        "\n",
        "L = nn.L1Loss()\n",
        "ssim_module = SSIM(data_range=255, size_average=True, channel=1)\n",
        "counter = 0\n",
        "loss = 0\n",
        "i_in = 0\n",
        "j_in = 0\n",
        "tot_val_loss = 0\n",
        "alpha = 20\n",
        "\n",
        "total_loss = 0\n",
        "\n",
        "final_loss = []\n",
        "final_mae = []\n",
        "final_ssim = []\n",
        "val_loss_arr = []\n",
        "\n",
        "\n",
        "if version == 0:\n",
        "    edge_len = 3\n",
        "elif version == 1 or version == 2:\n",
        "    edge_len = 4\n",
        "elif version == 3:\n",
        "    edge_len = 5\n",
        "elif version == 4:\n",
        "    edge_len = 6\n",
        "elif version == 5:\n",
        "    edge_len = 5\n",
        "elif version == 6:\n",
        "    edge_len = 5\n",
        "elif version == 7 or version == 8 or version == 9 or version == 10 or version == 11:\n",
        "    edge_len = 6\n",
        "\n",
        "if os.path.isfile(PATH + \"GTmodel.pt\"):\n",
        "    print('loading model...')\n",
        "    checkpoint = torch.load(PATH + \"GTmodel.pt\")\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    i_in = checkpoint['epoch']\n",
        "    total_loss = checkpoint['loss']\n",
        "    j_in = checkpoint['batch']\n",
        "\n",
        "    if os.path.isfile(PATH + 'train_loss.npy'):\n",
        "        final_loss = np.load(PATH + 'train_loss.npy')\n",
        "        final_loss = final_loss.tolist()\n",
        "    if os.path.isfile(PATH + 'val_loss.npy'):\n",
        "        val_loss_arr = np.load(PATH + 'val_loss.npy')\n",
        "        val_loss_arr = val_loss_arr.tolist()\n",
        "\n",
        "\n",
        "print(final_loss)\n",
        "print(val_loss_arr)\n",
        "\n",
        "for i in range(i_in, epochs):\n",
        "\n",
        "        lstbatch = []\n",
        "        print('****')\n",
        "\n",
        "        np.random.shuffle(lsttrain)\n",
        "        np.random.shuffle(lstval)\n",
        "\n",
        "        for j in range(j_in, n_batches):\n",
        "\n",
        "            print(\"batch n.\", j+1, \"/\", n_batches)\n",
        "            if (j+1)*batch_size < len(lsttrain):\n",
        "                lstbatch = lsttrain[j*batch_size:(j+1)*batch_size]\n",
        "            else:\n",
        "                lstbatch = lsttrain[j*batch_size:]\n",
        "\n",
        "            counter = 0\n",
        "\n",
        "\n",
        "            for filename in lstbatch:\n",
        "\n",
        "                #print(filename)\n",
        "\n",
        "                filename = os.path.splitext(filename)[0]\n",
        "                filename = filename[:-3]\n",
        "                #print(filename)\n",
        "                loss1 = 0\n",
        "                loss2 = 0\n",
        "\n",
        "\n",
        "                nodes, edges, mask, l = get_nodes_and_edges(filename, train_feat_paths, node_n, edge_n)\n",
        "\n",
        "                #print(len(nodes))\n",
        "                #print(\"TIME TO EXTRACT: \", time.time() - start)\n",
        "\n",
        "\n",
        "                nodes = nodes.to(device)\n",
        "                edges = edges.to(device)\n",
        "                mask = mask.to(device)\n",
        "\n",
        "\n",
        "                nodes_new, edges_new = GT(nodes, edges, mask = mask)\n",
        "\n",
        "\n",
        "                nodes_new = nodes_new.to(device)\n",
        "                edges_new = edges_new.to(device)\n",
        "\n",
        "\n",
        "                coord = projector3D(nodes_new, nodes)\n",
        "\n",
        "                #print(coord.shape)\n",
        "                #noise = np.random.normal(0,1,[coord.shape[1], 3])\n",
        "                #coord = coord + torch.from_numpy(noise).to(device)\n",
        "\n",
        "\n",
        "                coord = coord.to(device)\n",
        "\n",
        "                Y = edges[:,:,:,edge_len]\n",
        "\n",
        "                X = torch.zeros(coord.shape[1], coord.shape[1])\n",
        "                pred_dist = torch.zeros(1, coord.shape[1], coord.shape[1])\n",
        "\n",
        "                Y = Y.to(device)\n",
        "                X = X.to(device)\n",
        "                #print(int(coord.shape[1]))\n",
        "\n",
        "\n",
        "                for p in range (0,int(coord.shape[1])):\n",
        "                    for q in range (0, p):\n",
        "                        X[p,q] = torch.linalg.norm(coord[0,p,:] -  coord[0,q,:])\n",
        "\n",
        "\n",
        "                pred_dist[0] = X + X.T - torch.diag(X)\n",
        "                #print(\"ELAPSED TIME to EVALUATE PRED MAP: \", time.time() - start)\n",
        "\n",
        "                #print(X)\n",
        "\n",
        "\n",
        "                pred_dist = pred_dist.to(device)\n",
        "\n",
        "\n",
        "                loss1 = L(pred_dist, Y)\n",
        "                loss2 = 1 - ssim_module(pred_dist.unsqueeze(0), Y.unsqueeze(0))\n",
        "                loss += loss1 + alpha*loss2\n",
        "\n",
        "\n",
        "                total_loss = total_loss +  (loss1 + alpha*loss2).item()\n",
        "                counter = counter + 1\n",
        "\n",
        "                del nodes, edges, nodes_new, edges_new, mask, coord, X, Y, pred_dist\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                torch.no_grad()\n",
        "\n",
        "            if i % 1 == 0 and j % 3 == 0:\n",
        "                 print('epoch: %d,  batch: %d,  avg loss: %.3f' % (i, j, loss.item()/batch_size))\n",
        "                 print('MAE: %.3f,  SSIM: %.3f' % ( loss1, 1-loss2))\n",
        "                 #torch.save(model.state_dict(), \"/content/model.pt\")\n",
        "\n",
        "                 torch.save({\n",
        "                      'epoch': i,\n",
        "                      'batch': j,\n",
        "                      'model_state_dict': model.state_dict(),\n",
        "                      'optimizer_state_dict': optimizer.state_dict(),\n",
        "                      'scheduler_state_dict': scheduler.state_dict(),\n",
        "                      'loss': total_loss,\n",
        "                      },  PATH + \"GTmodel.pt\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                 print('model saved!')\n",
        "\n",
        "            loss = loss / batch_size\n",
        "            start = time.time()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss = 0\n",
        "\n",
        "\n",
        "        ####################################Validation\n",
        "        print(\"Validation\")\n",
        "        tot_val_loss = 0\n",
        "        CNT = 0\n",
        "\n",
        "        for filename in lstval:\n",
        "            #print(filename)\n",
        "            CNT += 1\n",
        "            print(CNT, \"/\", len(lstval))\n",
        "\n",
        "\n",
        "            filename = os.path.splitext(filename)[0]\n",
        "            filename = filename[:-3]\n",
        "\n",
        "            nodes, edges, mask, l = get_nodes_and_edges(filename, train_feat_paths, node_n, edge_n)\n",
        "            nodes = nodes.to(device)\n",
        "            edges = edges.to(device)\n",
        "            mask = mask.to(device)\n",
        "\n",
        "            nodes_new, edges_new = GT(nodes, edges, mask = mask)\n",
        "\n",
        "            coord = projector3D(nodes_new, nodes)\n",
        "\n",
        "            Y = edges[:,:,:,edge_len]\n",
        "\n",
        "            #print(coord.shape[1])\n",
        "            #print(filename)\n",
        "            #print(\"+++\")\n",
        "\n",
        "            X = torch.zeros(coord.shape[1], coord.shape[1])\n",
        "            pred_dist = torch.zeros(1, coord.shape[1], coord.shape[1])\n",
        "\n",
        "            for p in range (0,int(coord.shape[1])):\n",
        "                    for q in range (0, p):\n",
        "                        X[p,q] = torch.linalg.norm(coord[0,p,:] -  coord[0,q,:])\n",
        "\n",
        "\n",
        "            pred_dist[0] = X + X.T - torch.diag(X)\n",
        "            #print(\"ELAPSED TIME to EVALUATE PRED MAP: \", time.time() - start)\n",
        "\n",
        "            pred_dist = pred_dist.to(device)\n",
        "\n",
        "            loss1 = L(pred_dist, Y)\n",
        "            loss2 = 1 - ssim_module(pred_dist.unsqueeze(0), Y.unsqueeze(0))\n",
        "\n",
        "            tot_val_loss = tot_val_loss +  (loss1 + alpha*loss2).item()\n",
        "            counter = counter + 1\n",
        "\n",
        "            del nodes, edges, nodes_new, edges_new, mask, l, coord, X, Y, pred_dist\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            torch.no_grad()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "        #print('validation loss: %.3f' % (tot_val_loss /len(lstval)))\n",
        "        val_loss_arr = np.append(val_loss_arr, (tot_val_loss /len(lstval)))\n",
        "        final_loss = np.append(final_loss, total_loss/len(lsttrain))\n",
        "\n",
        "        print(\"....\")\n",
        "        print(\"validation loss:\", val_loss_arr)\n",
        "        print(\"training loss:\", final_loss)\n",
        "        print(\"....\")\n",
        "\n",
        "\n",
        "        np.save(PATH + 'train_loss.npy', final_loss)\n",
        "        np.save(PATH +'val_loss.npy', val_loss_arr)\n",
        "\n",
        "        if tot_val_loss /len(lstval) < minimum:\n",
        "            print(\"FOUND A NEW BEST!!\")\n",
        "            torch.save({\n",
        "                      'epoch': i,\n",
        "                      'batch': j,\n",
        "                      'model_state_dict': model.state_dict(),\n",
        "                      'optimizer_state_dict': optimizer.state_dict(),\n",
        "                      'scheduler_state_dict': scheduler.state_dict(),\n",
        "                      'loss': loss,\n",
        "                      'minimum':tot_val_loss /len(lstval),\n",
        "                      },  PATH + \"BEST_GTmodel.pt\")\n",
        "            minimum = tot_val_loss /len(lstval)\n",
        "\n",
        "        if early_stopper.early_stop(tot_val_loss/len(lstval)):\n",
        "                stopflag = 1\n",
        "                break\n",
        "\n",
        "        tot_val_loss = 0\n",
        "        total_loss = 0\n",
        "\n",
        "        if j_in != 0:\n",
        "            j_in = 0\n",
        "\n",
        "        if (i + 1) % 2 == 0:\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "        if stopflag == 1:\n",
        "            break\n",
        "\n",
        "            #print(\"ELAPSED TIME to UPDATE WEIGHTS: \", time.time() - start)\n",
        "            #if i % 1 == 0 and j %  == 0:\n",
        "\n",
        "\n",
        "        counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TvCTnQluJGaA"
      },
      "outputs": [],
      "source": [
        "val_loss_arr = np.append(val_loss_arr, (tot_val_loss /len(lstval)))\n",
        "final_loss = np.append(final_loss, total_loss/len(lsttrain))\n",
        "\n",
        "print(\"....\")\n",
        "print(\"validation loss:\", val_loss_arr)\n",
        "print(\"training loss:\", final_loss)\n",
        "print(\"....\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RXd13pB1nsui"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"/content/model.pt\")\n",
        "print('model saved!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2RPK5OaGG7Ol"
      },
      "outputs": [],
      "source": [
        "np.save(PATH +'train_loss.npy', final_loss)\n",
        "np.save(PATH +'val_loss.npy', val_loss_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "xlanY7nRypdI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4pSIRW5JCydF"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "dir = '/content/drive/MyDrive/DISTANCES/'\n",
        "lst = os.listdir(dir)\n",
        "lst.sort()\n",
        "\n",
        "test_feat_paths = [dir_dataset + '/psicov/features/']\n",
        "\n",
        "if version == 0:\n",
        "    edge_len = 3\n",
        "elif version == 1 or version == 2:\n",
        "    edge_len = 4\n",
        "elif version == 3:\n",
        "    edge_len = 5\n",
        "elif version == 4:\n",
        "    edge_len = 6\n",
        "elif version == 5:\n",
        "    edge_len = 5\n",
        "elif version == 6:\n",
        "    edge_len = 5\n",
        "elif version == 7 or version == 8 or version == 9 or version == 10 or version == 11:\n",
        "    edge_len = 6\n",
        "\n",
        "\n",
        "dir_dataset = './data/'\n",
        "\n",
        "projector3D = Net()\n",
        "#GT.cuda()\n",
        "#projector3D.cuda()\n",
        "model = nn.Sequential(GT, projector3D)\n",
        "#model.cuda()\n",
        "\n",
        "\n",
        "L = nn.L1Loss()\n",
        "\n",
        "if os.path.isfile(PATH + \"BEST_GTmodel.pt\"):\n",
        "    print('loading model...')\n",
        "    checkpoint = torch.load(PATH + \"BEST_GTmodel.pt\")\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "total_loss = []\n",
        "coordinates_array = []\n",
        "protein_length = []\n",
        "similarity = []\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for filename in lst_test:\n",
        "    filename = os.path.splitext(filename)[0]\n",
        "    filename = filename[:-3]\n",
        "\n",
        "    nodes = []\n",
        "    edges = []\n",
        "    mask = []\n",
        "    coord = []\n",
        "    nodes_out = []\n",
        "\n",
        "    nodes, edges, mask, l = get_nodes_and_edges(filename, test_feat_paths, node_n, edge_n)\n",
        "\n",
        "    #nodes = nodes.cuda()\n",
        "    #edges = edges.cuda()\n",
        "    #mask = mask.cuda()\n",
        "\n",
        "    '''\n",
        "    if counter < 5:\n",
        "      print(nodes)\n",
        "    '''\n",
        "\n",
        "    nodes_new, edges = GT(nodes, edges, mask = mask)\n",
        "    #nodes_new = nodes_new.cuda()\n",
        "    coord = projector3D(nodes_new, nodes)\n",
        "\n",
        "\n",
        "    '''\n",
        "    if counter < 5:\n",
        "      print('Output Nodes:')\n",
        "      print(nodes_new)\n",
        "      print('---')\n",
        "      counter += 1\n",
        "    '''\n",
        "    #coord = coord.detach_()\n",
        "\n",
        "    or_dist = edges[:,:,:,edge_len]\n",
        "\n",
        "    pred_dist = torch.zeros(1, coord.shape[1], coord.shape[1])\n",
        "    #pred_dist = np.zeros([coord.shape[1], coord.shape[1]])\n",
        "\n",
        "\n",
        "    for p in range (0,int(coord.shape[1])):\n",
        "      for q in range (0,p):\n",
        "        pred_dist[0, p,q] = torch.linalg.norm(coord[0,p,:] -  coord[0,q,:])\n",
        "\n",
        "\n",
        "    pred_dist[0] = pred_dist[0] + pred_dist[0].T - torch.diag(torch.diag(pred_dist[0]))\n",
        "\n",
        "\n",
        "    #or_dist = or_dist.cuda()\n",
        "    #pred_dist = pred_dist.cuda()\n",
        "\n",
        "    #print(coord)\n",
        "\n",
        "    loss = L(pred_dist, or_dist)\n",
        "\n",
        "    img = or_dist.cpu().detach().numpy()[0][0]\n",
        "    ssimerror = ssim(pred_dist.cpu().detach().numpy()[0][0], img, data_range=img.max() - img.min())\n",
        "\n",
        "    a = loss.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "    total_loss = np.append(total_loss, a)\n",
        "\n",
        "\n",
        "    print(np.asscalar(a))\n",
        "    #print(counter)\n",
        "    #counter += 1\n",
        "\n",
        "\n",
        "    if np.asscalar(a) < 3.7  and counter < 8:\n",
        "        print(\"MAE:  \", np.asscalar(a))\n",
        "        print(\"SSIM: \", ssimerror)\n",
        "        print(filename)\n",
        "        print('****')\n",
        "        #print(pred_dist[0])\n",
        "        plt.figure()\n",
        "        b = pred_dist.cpu().detach().numpy()\n",
        "        imshow(np.asarray(b[0]), cmap = \"plasma\")\n",
        "\n",
        "\n",
        "        plt.figure()\n",
        "        b = or_dist.cpu().detach().numpy()\n",
        "        imshow(np.asarray(b[0]), cmap = \"plasma\")\n",
        "\n",
        "        counter = counter + 1\n",
        "\n",
        "        coordinates_array = np.append(coordinates_array, np.asarray(coord.cpu().detach().numpy()))\n",
        "        protein_length = np.append(protein_length, int(coord.shape[1]))\n",
        "\n",
        "    similarity = np.append(similarity, ssimerror)\n",
        "    #print(similarity)\n",
        "\n",
        "\n",
        "    del nodes, edges, mask, coord, or_dist, pred_dist\n",
        "    gc.collect()\n",
        "\n",
        "print(similarity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4U6JZQRGsgnm"
      },
      "outputs": [],
      "source": [
        "#np.save('coordinates.npy', coordinates_array)\n",
        "# np.save('length.npy', protein_length)\n",
        "np.save(PATH + 'loss.npy', total_loss)\n",
        "np.save(PATH + 'similarity.npy', similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lcLfqvendJtt"
      },
      "outputs": [],
      "source": [
        "print(np.max(total_loss))\n",
        "print(np.median(total_loss))\n",
        "print(np.min(total_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IUUasvi5e6HU"
      },
      "outputs": [],
      "source": [
        "print(np.max(similarity))\n",
        "print(np.median(similarity))\n",
        "print(np.min(similarity))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Verifying the alignment on selected proteins"
      ],
      "metadata": {
        "id": "ny5ag_UHysA1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c-35Vu1mekue"
      },
      "outputs": [],
      "source": [
        "#lsttest = lst\n",
        "#index = np.argmin(total_loss)\n",
        "#lsttest = lsttest[index]\n",
        "counter = 0\n",
        "\n",
        "types = [\"1mk0A-cb.npy\", \"1z0jB-cb.npy\", \"1yqhA-cb.npy\", \"1zv1A-cb.npy\", \"2d0oB-cb.npy\", \"2dgbA-cb.npy\", \"2dm9A-cb.npy\", \"2ehwA-cb.npy\", \"2fyuK-cb.npy\", \"2fztA-cb.npy\", \"2gomA-cb.npy\"]\n",
        "\n",
        "all_feat_paths = [dir_dataset + '/deepcov/features/', dir_dataset + '/psicov/features/', dir_dataset + '/cameo/features/']\n",
        "\n",
        "\n",
        "while counter < 10:\n",
        "\n",
        "    lsttest = types[counter]\n",
        "    counter += 1\n",
        "\n",
        "    filename = os.path.splitext(lsttest)[0]\n",
        "    filename = filename[:-3]\n",
        "\n",
        "    print(filename)\n",
        "\n",
        "    nodes = []\n",
        "    edges = []\n",
        "    mask = []\n",
        "    coord = []\n",
        "    nodes_out = []\n",
        "\n",
        "    nodes, edges, mask, l = get_nodes_and_edges(filename, all_feat_paths, node_n, edge_n)\n",
        "\n",
        "    #nodes = nodes.cuda()\n",
        "    #edges = edges.cuda()\n",
        "    #mask = mask.cuda()\n",
        "\n",
        "\n",
        "    nodes_new, edges = GT(nodes, edges, mask = mask)\n",
        "    #nodes_new = nodes_new.cuda()\n",
        "    coord = projector3D(nodes_new, nodes)\n",
        "\n",
        "\n",
        "    #coord = coord.detach_()\n",
        "\n",
        "    or_dist = edges[:,:,:,edge_len]\n",
        "\n",
        "\n",
        "    pred_dist = torch.zeros(1, coord.shape[1], coord.shape[1])\n",
        "    #pred_dist = np.zeros([coord.shape[1], coord.shape[1]])\n",
        "\n",
        "\n",
        "    for p in range (0,int(coord.shape[1])):\n",
        "      for q in range (0,p):\n",
        "        pred_dist[0, p,q] = torch.linalg.norm(coord[0,p,:] -  coord[0,q,:])\n",
        "\n",
        "\n",
        "    pred_dist[0] = pred_dist[0] + pred_dist[0].T - torch.diag(torch.diag(pred_dist[0]))\n",
        "\n",
        "\n",
        "    #or_dist = or_dist.cuda()\n",
        "    #pred_dist = pred_dist.cuda()\n",
        "\n",
        "    #print(coord)\n",
        "\n",
        "    loss = L(pred_dist, or_dist)\n",
        "    a = loss.cpu().detach().numpy()\n",
        "\n",
        "    img = or_dist.cpu().detach().numpy()[0][0]\n",
        "    ssimerror = ssim(pred_dist.cpu().detach().numpy()[0][0], img, data_range=img.max() - img.min())\n",
        "\n",
        "    print(np.asscalar(a))\n",
        "    print(ssimerror)\n",
        "    print()\n",
        "    plt.figure()\n",
        "    b = pred_dist.cpu().detach().numpy()\n",
        "    imshow(np.asarray(b[0]))\n",
        "\n",
        "\n",
        "    plt.figure()\n",
        "    b = or_dist.cpu().detach().numpy()\n",
        "    imshow(np.asarray(b[0]))\n",
        "\n",
        "    np.save(PATH + \"version\" + str(version) +'/coordinates' + str(filename) + '_version' + str(version) + '.npy', coord.cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w5RU6zkZF0Qx"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "dir = '/content/drive/MyDrive/DISTANCES/'\n",
        "lst = os.listdir(dir)\n",
        "lst.sort()\n",
        "lsttest = lst[200:950]\n",
        "\n",
        "namelist = []\n",
        "for filename in lsttest:\n",
        "    print(filename)\n",
        "    filename = os.path.splitext(filename)[0]\n",
        "    filename = filename[:-4]\n",
        "\n",
        "    namelist = np.append(namelist, filename)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9l4kvzy689SL"
      },
      "outputs": [],
      "source": [
        "#!zip -r /content/version1.zip /content/version1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwmaQKr7nYgd"
      },
      "source": [
        "# Evaluating GDT scores over the Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9dBMZ2xZep6C"
      },
      "outputs": [],
      "source": [
        "def rigid_transform_3D(A, B):\n",
        "    assert A.shape == B.shape\n",
        "\n",
        "    num_rows, num_cols = A.shape\n",
        "    if num_rows != 3:\n",
        "        raise Exception(f\"matrix A is not 3xN, it is {num_rows}x{num_cols}\")\n",
        "\n",
        "    num_rows, num_cols = B.shape\n",
        "    if num_rows != 3:\n",
        "        raise Exception(f\"matrix B is not 3xN, it is {num_rows}x{num_cols}\")\n",
        "\n",
        "    # find mean column wise\n",
        "    centroid_A = np.mean(A, axis=1)\n",
        "    centroid_B = np.mean(B, axis=1)\n",
        "\n",
        "    # ensure centroids are 3x1\n",
        "    centroid_A = centroid_A.reshape(-1, 1)\n",
        "    centroid_B = centroid_B.reshape(-1, 1)\n",
        "\n",
        "    # subtract mean\n",
        "    Am = A - centroid_A\n",
        "    Bm = B - centroid_B\n",
        "\n",
        "    H = Am @ np.transpose(Bm)\n",
        "\n",
        "    # sanity check\n",
        "    #if linalg.matrix_rank(H) < 3:\n",
        "    #    raise ValueError(\"rank of H = {}, expecting 3\".format(linalg.matrix_rank(H)))\n",
        "\n",
        "    # find rotation\n",
        "    U, S, Vt = np.linalg.svd(H)\n",
        "    R = Vt.T @ U.T\n",
        "\n",
        "    # special reflection case\n",
        "    if np.linalg.det(R) < 0:\n",
        "        #print(\"det(R) < R, reflection detected!, correcting for it ...\")\n",
        "        Vt[2,:] *= -1\n",
        "        R = Vt.T @ U.T\n",
        "\n",
        "    t = -R @ centroid_A + centroid_B\n",
        "\n",
        "    return R, t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6CVWdyU3jXln"
      },
      "outputs": [],
      "source": [
        "!pip install biopython\n",
        "!pip install git+https://github.com/pygae/clifford.git@master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H-pbaLhSjPUm"
      },
      "outputs": [],
      "source": [
        "from Bio.PDB.PDBParser import PDBParser\n",
        "from clifford.tools.g3c import *\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lmXLWwNzOydy"
      },
      "outputs": [],
      "source": [
        "dir = '/content/drive/MyDrive/DISTANCES/'\n",
        "lst = os.listdir(dir)\n",
        "lst.sort()\n",
        "\n",
        "dir = '/content/drive/MyDrive/coordinates/'\n",
        "lst = os.listdir(dir)\n",
        "lst.sort()\n",
        "\n",
        "dir_dataset = './data/'\n",
        "all_feat_paths = [dir_dataset + '/psicov/features/']\n",
        "\n",
        "model_NN = nn.Sequential(GT, projector3D)\n",
        "#model_NN.cuda()\n",
        "\n",
        "if os.path.isfile('/content/model.pt'):\n",
        "    print('loading model...')\n",
        "    model_NN.load_state_dict(torch.load(\"/content/model.pt\"))\n",
        "\n",
        "\n",
        "parser = PDBParser(PERMISSIVE=1)\n",
        "structure_id = \"chain\"\n",
        "\n",
        "GDT_TS_TOTAL = []\n",
        "GDT_HA_TOTAL = []\n",
        "\n",
        "for filename in lst_test:\n",
        "    filename = os.path.splitext(filename)[0]\n",
        "    filename = filename[:-3]\n",
        "    #print(filename)\n",
        "\n",
        "    nodes = []\n",
        "    edges = []\n",
        "    mask = []\n",
        "    coord = []\n",
        "    nodes_out = []\n",
        "\n",
        "    nodes, edges, mask, l = get_nodes_and_edges(filename, all_feat_paths, node_n, edge_n)\n",
        "\n",
        "    #nodes = nodes.cuda()\n",
        "    #edges = edges.cuda()\n",
        "    #mask = mask.cuda()\n",
        "\n",
        "\n",
        "    nodes_new, edges = GT(nodes, edges, mask = mask)\n",
        "    #nodes_new = nodes_new.cuda()\n",
        "    pred_coord = projector3D(nodes_new, nodes)\n",
        "\n",
        "\n",
        "    pred_coord = pred_coord.detach().cpu().numpy()\n",
        "\n",
        "    target_chain = filename[-1]\n",
        "\n",
        "    #print(target_chain)\n",
        "    print(filename)\n",
        "    filename = filename[:-1]\n",
        "\n",
        "\n",
        "    if 'pdb'+ filename + '.ent' in lst:\n",
        "        #print('pdb'+ filename + '.ent')\n",
        "\n",
        "        structure = parser.get_structure(structure_id, dir +'pdb' + filename + '.ent')\n",
        "\n",
        "        N = 0\n",
        "        m = 0\n",
        "        I = e1*e2*e3\n",
        "\n",
        "\n",
        "        idx = 0\n",
        "        TOL =  15 #MEASURE IN ANGSTROM DEFINING THE RADIUS OF INTERACTION BETWEEN TWO RESIDUES\n",
        "        chain_n = 0\n",
        "\n",
        "        #counting the total number of atoms N in the chain\n",
        "        for model in structure:\n",
        "            for chain in model:\n",
        "                chain_n = chain_n + 1\n",
        "                #print(chain.id)\n",
        "                #if chain.id ==  'A':\n",
        "                for residue in chain:\n",
        "                    for atom in residue:\n",
        "                        #print(atom)\n",
        "                        if atom.altloc == \"B\":\n",
        "                            #print(atom)\n",
        "                            del atom\n",
        "\n",
        "\n",
        "        for model in structure:\n",
        "            for chain in model:\n",
        "                #print(chain.id)\n",
        "                #if chain.id ==  'A':\n",
        "                for residue in chain:\n",
        "                    for atom in residue:\n",
        "                        if chain.id ==  target_chain:\n",
        "                            N = N + 1\n",
        "                            if atom.name == \"CA\":\n",
        "                                idx = idx + 1\n",
        "\n",
        "\n",
        "        #storing 3D coordinates in an array P for each of the atoms\n",
        "        P = np.zeros([N, 3])\n",
        "        cnt = np.zeros([idx, 1])\n",
        "\n",
        "\n",
        "        i = 0\n",
        "        m = 0\n",
        "        for model in structure:\n",
        "            for chain in model:\n",
        "                if chain.id == target_chain:\n",
        "                  for residue in chain:\n",
        "                    for atom in residue:\n",
        "                        if chain.id == target_chain:\n",
        "                            P[m] = atom.get_coord()\n",
        "                            m = m + 1;\n",
        "                            if atom.name == \"CA\":\n",
        "                                cnt[i,0] = m\n",
        "                                i = i + 1\n",
        "\n",
        "\n",
        "        coord = []\n",
        "\n",
        "\n",
        "        for m in range(0,idx):\n",
        "            i = int(cnt[m,0])-1\n",
        "            coord = np.append(coord, P[i,:])\n",
        "\n",
        "        coord = np.reshape(coord, [idx, 3])\n",
        "\n",
        "\n",
        "        sum_x = 0\n",
        "        sum_y = 0\n",
        "        sum_z = 0\n",
        "\n",
        "        for i in range(0, idx):\n",
        "            sum_x += coord[i, 0]\n",
        "            sum_y += coord[i, 1]\n",
        "            sum_z += coord[i, 2]\n",
        "\n",
        "        M = [sum_x/idx, sum_y/idx, sum_z/idx]\n",
        "\n",
        "        coord = coord - M\n",
        "\n",
        "\n",
        "        sum_x = 0\n",
        "        sum_y = 0\n",
        "        sum_z = 0\n",
        "\n",
        "        if idx > pred_coord.shape[1]:\n",
        "          idx = pred_coord.shape[1]\n",
        "          coord = coord[:idx]\n",
        "\n",
        "        for i in range(0, idx):\n",
        "            sum_x += pred_coord[0, i, 0]\n",
        "            sum_y += pred_coord[0, i, 1]\n",
        "            sum_z += pred_coord[0, i, 2]\n",
        "\n",
        "        M = [sum_x/idx, sum_y/idx, sum_z/idx]\n",
        "\n",
        "        pred_coord[0] = pred_coord[0] - M\n",
        "\n",
        "\n",
        "        GDT_TS_max = 0\n",
        "        GDT_HA_max = 0\n",
        "        index = 0\n",
        "        best_X = coord\n",
        "\n",
        "        for k in range(0, 2000):\n",
        "\n",
        "            if k == 0:\n",
        "                R, t = rigid_transform_3D(coord.reshape((3, idx)), pred_coord[0].reshape((3, idx)))\n",
        "                X = np.matmul(coord, R)\n",
        "                #X = coord\n",
        "                #Y = np.matmul(pred_coord[0], R)\n",
        "                Y = pred_coord[0]\n",
        "\n",
        "            else :\n",
        "                R, t = rigid_transform_3D(X.reshape((3, idx)), Y.reshape((3, idx)))\n",
        "                X = np.matmul(X, R)\n",
        "                #X = X\n",
        "                Y = Y\n",
        "                #Y = np.matmul(Y, R)\n",
        "\n",
        "            total_1 = 0\n",
        "            total_2 = 0\n",
        "            total_4 = 0\n",
        "            total_8 = 0\n",
        "\n",
        "            for i in range(0, idx):\n",
        "                distance = np.linalg.norm(X[i,:] - Y[i, :])\n",
        "                if distance < 1:\n",
        "                    total_1 += 1\n",
        "                if distance < 2:\n",
        "                    total_2 += 1\n",
        "                if distance < 4:\n",
        "                    total_4 += 1\n",
        "                if distance < 8:\n",
        "                    total_8 += 1\n",
        "\n",
        "            GDT_P1 = total_1 / idx\n",
        "            GDT_P2 = total_2 / idx\n",
        "            GDT_P4 = total_4 / idx\n",
        "            GDT_P8 = total_8 / idx\n",
        "\n",
        "            GDT_TS = (GDT_P1 + GDT_P2 + GDT_P4 +GDT_P8)*100/4\n",
        "\n",
        "            total_1 = 0\n",
        "            total_2 = 0\n",
        "            total_4 = 0\n",
        "            total_8 = 0\n",
        "\n",
        "            for i in range(0, idx):\n",
        "                distance = np.linalg.norm(X[i,:] - Y[i, :])\n",
        "                if distance < 1/2:\n",
        "                    total_1 += 1\n",
        "                if distance < 2/2:\n",
        "                    total_2 += 1\n",
        "                if distance < 4/2:\n",
        "                    total_4 += 1\n",
        "                if distance < 8/2:\n",
        "                    total_8 += 1\n",
        "\n",
        "            GDT_P1 = total_1 / idx\n",
        "            GDT_P2 = total_2 / idx\n",
        "            GDT_P4 = total_4 / idx\n",
        "            GDT_P8 = total_8 / idx\n",
        "\n",
        "            GDT_HA = (GDT_P1 + GDT_P2 + GDT_P4 +GDT_P8)*100/4\n",
        "\n",
        "            if GDT_TS > GDT_TS_max:\n",
        "              GDT_TS_max = GDT_TS\n",
        "              index = k\n",
        "              GDT_HA_max = GDT_HA\n",
        "              best_X = X\n",
        "\n",
        "        #print('maximum GDT_TS: ', GDT_TS_max)\n",
        "        #print('GDT_HA: ', GDT_HA_max)\n",
        "        #print('achieved after n. of iterations equal to:', index)\n",
        "        #print(\"****\")\n",
        "\n",
        "        if GDT_TS_max > 30:\n",
        "          print(filename)\n",
        "          print(GDT_TS_max)\n",
        "          print(idx)\n",
        "          print(\"******\")\n",
        "\n",
        "        GDT_TS_TOTAL = np.append(GDT_TS_TOTAL, GDT_TS_max)\n",
        "        GDT_HA_TOTAL = np.append(GDT_HA_TOTAL, GDT_HA_max)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kL45Iy1wid1d"
      },
      "outputs": [],
      "source": [
        "np.save(PATH + 'GDT_TS' + '.npy', GDT_TS_TOTAL)\n",
        "np.save(PATH + 'GDT_HA' + '.npy', GDT_HA_TOTAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "alflQMTfO2uz"
      },
      "outputs": [],
      "source": [
        "print(np.max(GDT_TS_TOTAL))\n",
        "print(np.median(GDT_TS_TOTAL))\n",
        "print(np.min(GDT_TS_TOTAL))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tdXJQhtdO3su"
      },
      "outputs": [],
      "source": [
        "print(np.max(GDT_HA_TOTAL))\n",
        "print(np.median(GDT_HA_TOTAL))\n",
        "print(np.min(GDT_HA_TOTAL))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Counting the number of trainable parameters"
      ],
      "metadata": {
        "id": "PUorWGV0y1Sh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hgBtpR2L10_q"
      },
      "outputs": [],
      "source": [
        "pytorch_total_params = sum(p.numel() for p in GT.parameters())\n",
        "pytorch_total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SQrfu8sD14tM"
      },
      "outputs": [],
      "source": [
        "pytorch_total_params = sum(p.numel() for p in projector3D.parameters())\n",
        "pytorch_total_params"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}